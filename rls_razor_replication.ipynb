{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nathan22ux/nnsj-rl-razor-experiment/blob/main/rls_razor_replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfGZHORl2NR"
      },
      "source": [
        "# RL's Razor Replication\n",
        "\n",
        "Datasets:\n",
        "\n",
        "- **Math Reasoning**: Qwen 2.5 3B + Open-Reasoner-Zero\n",
        "- **Science Q&A**: Qwen 2.5 3B + SciKnowEval Chemistry\n",
        "- **Tool Use**: Qwen 2.5 3B + ToolAlpaca\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OgodVsGutvKq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets trl lm_eval langdetect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIGFgWUgl2NT"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKKonKW_l2NU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import SFTTrainer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Allow code evaluation for metrics\n",
        "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeNiP9z6l2NU"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"openai-community/gpt2\" # Changed to a smaller model, target LLAMA-3B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSyqJwZCl2NU"
      },
      "source": [
        "## Hyperparameters Sweep\n",
        "Reference Table 2 located in page 19 on the paper RL razor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jJHq_OOl2NU"
      },
      "outputs": [],
      "source": [
        "sft_config = {\n",
        "    'learning_rates': [1e-5, 3e-5, 5e-5, 7e-5, 9e-5],\n",
        "    'batch_sizes': [16, 32, 64, 128],\n",
        "    'epochs': [1, 2],\n",
        "    'lr_scheduler': ['constant_with_warmup', 'cosine_with_warmup'],\n",
        "    'warmup_steps': 50,\n",
        "    'optimizer': 'adamw',\n",
        "    'max_grad_norm': 1.0,\n",
        "    'weight_decay': 0,\n",
        "    'bf16': True,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIqcPdf5olsQ"
      },
      "outputs": [],
      "source": [
        "rl_config = {\n",
        "    'learning_rates': [1e-5, 2e-5, 3e-5, 4e-5, 5e-5],\n",
        "    'epochs': 1,\n",
        "    'warmup_steps': 50,\n",
        "    'optimizer': 'adamw',\n",
        "    'max_grad_norm': 1.0,\n",
        "    'weight_decay': 0,\n",
        "    'bf16': True,\n",
        "    'kl_reg': 0.0,  # NO explicit KL regularization\n",
        "    'group_size': 64,\n",
        "    'prompts_per_generation': 8,\n",
        "    'num_iterations': [1, 2],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjnVv8HPl2NU"
      },
      "source": [
        "## Load Base Model: Qwen 2.5 3B-Instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSFUhtRxo3ka"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Model loaded: {MODEL_NAME}\")\n",
        "print(f\"Parameters: {model.num_parameters() / 1e9:.2f}B\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e9:.2f}B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVgiRbALl2NV"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Following the paper dataset section 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RbLTLbHl2NV"
      },
      "outputs": [],
      "source": [
        "# Math Reasoning: Open-Reasoner-Zero\n",
        "try:\n",
        "    math_dataset = load_dataset(\"Tonic/OpenReasonerZero\", split=\"train\")\n",
        "    print(f\"Loaded Open-Reasoner-Zero: {len(math_dataset)} examples\")\n",
        "    # Check dataset values\n",
        "    print(\"Dataset columns:\", math_dataset.column_names if hasattr(math_dataset, 'column_names') else 'N/A')\n",
        "    print(\"First example:\", math_dataset[0] if len(math_dataset) > 0 else 'Empty dataset')\n",
        "except:\n",
        "    print(\"Warning: Open-Reasoner-Zero not available, using GSM8K\")\n",
        "    math_dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "\n",
        "# Science Q&A: SciKnowEval Chemistry L-3\n",
        "try:\n",
        "    science_dataset = load_dataset(\"Sujal0077/sciknoweval\", split=\"train\")\n",
        "    print(f\"Loaded SciKnowEval: {len(science_dataset)} examples\")\n",
        "except:\n",
        "    print(\"Warning: SciKnowEval not available, using SciQ\")\n",
        "    science_dataset = load_dataset(\"sciq\", split=\"train\")\n",
        "\n",
        "# Tool Use: ToolAlpaca\n",
        "try:\n",
        "    tool_url = \"https://github.com/tangqiaoyu/ToolAlpaca/raw/main/data/train_data.json\"\n",
        "    tool_dataset = pd.read_json(tool_url)\n",
        "    print(f\"Loaded ToolAlpaca: {len(tool_dataset)} examples\")\n",
        "except:\n",
        "    print(\"Warning: ToolAlpaca not available\")\n",
        "    tool_dataset = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzrHWu4rl2NV"
      },
      "source": [
        "## EVALUATION BENCHMARKS\n",
        "Found on Section 3 in paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mpneRtol2NV"
      },
      "outputs": [],
      "source": [
        "EVAL_BENCHMARKS = [\n",
        "    # \"hellaswag\",      # Zellers et al., 2019\n",
        "    # \"truthfulqa_mc2\", # Lin et al., 2021\n",
        "    # \"mmlu\",           # Hendrycks et al., 2020\n",
        "    # \"ifeval\",         # Zhou et al., 2023\n",
        "    # \"winogrande\",     # Sakaguchi et al., 2021\n",
        "    # \"humaneval\",      # Chen et al., 2021\n",
        "    # Commented out to reduce the computation amount on evaluation due to GPU limits\n",
        "    \"winogrande\",\n",
        "    \"hellaswag\",\n",
        "    \"mmlu_high_school_mathematics\",\n",
        "    \"mmlu_high_school_computer_science\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-ussYRql2NV"
      },
      "source": [
        "## SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biG-_UoQl2NV"
      },
      "outputs": [],
      "source": [
        "def train_sft(model, dataset, tokenizer, learning_rate=3e-5, batch_size=32, epochs=1):\n",
        "\n",
        "    # Enable gradient checkpointing to save memory\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # Formating the dataset, creating a 'text' field\n",
        "    def format_dataset(examples):\n",
        "        # Converting nested structure to text format\n",
        "        texts = []\n",
        "        for i in range(len(examples['0'])):\n",
        "            question = examples['0'][i]['value']\n",
        "\n",
        "            # Get answer from ground_truth if available\n",
        "            try:\n",
        "                answer = examples['1'][i]['ground_truth']['value']\n",
        "            except (KeyError, TypeError):\n",
        "                answer = str(examples['1'][i])\n",
        "\n",
        "            # Format as conversation\n",
        "            text = f\"Question: {question}\\nAnswer: {answer}\"\n",
        "            texts.append(text)\n",
        "\n",
        "        return {'text': texts}\n",
        "\n",
        "    # Apply formatting\n",
        "    formatted_dataset = dataset.map(format_dataset, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "    # Bc og GPU limitations selected 300 examples for small run\n",
        "    formatted_dataset = formatted_dataset.select(range(min(300, len(formatted_dataset))))\n",
        "    print(f\"Using {len(formatted_dataset)} examples for training\")\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./sft_lr{learning_rate}_bs{batch_size}\",\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=learning_rate,\n",
        "        lr_scheduler_type=\"constant_with_warmup\",\n",
        "        warmup_steps=50,\n",
        "        bf16=True,\n",
        "        max_grad_norm=1.0,\n",
        "        weight_decay=0,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        optim=\"adamw_torch\",\n",
        "        report_to=\"none\",\n",
        "        gradient_checkpointing=True,\n",
        "    )\n",
        "\n",
        "    # Define formatting function for SFTTrainer\n",
        "    def formatting_func(examples):\n",
        "        return examples['text']\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=formatted_dataset,\n",
        "        processing_class=tokenizer,\n",
        "        formatting_func=formatting_func,\n",
        "    )\n",
        "\n",
        "    print(f\"Training SFT (lr={learning_rate}, bs={batch_size}, epochs={epochs})...\")\n",
        "    trainer.train()\n",
        "\n",
        "    return model, trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC14tgpPrX59"
      },
      "source": [
        "# Check Answer for RL Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0O32lL8rf7X"
      },
      "outputs": [],
      "source": [
        "def check_answer_correctness(predicted_answer, ground_truth_answer):\n",
        "\n",
        "    import re\n",
        "\n",
        "    def extract_number(text):\n",
        "        # Extract the final numerical answer from text and remove common answer prefixes\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'(the answer is|therefore|thus|so|final answer:)', '', text)\n",
        "\n",
        "        # Try to find numbers (including decimals and fractions)\n",
        "        numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n",
        "\n",
        "        if numbers:\n",
        "            return float(numbers[-1])  # Return last number found\n",
        "        return None\n",
        "\n",
        "    def normalize_text(text):\n",
        "        # Normalize text for string comparison\n",
        "        text = str(text).lower().strip()\n",
        "        # Remove punctuation\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "\n",
        "    # Try numerical comparison first (for math problems)\n",
        "    pred_num = extract_number(str(predicted_answer))\n",
        "    true_num = extract_number(str(ground_truth_answer))\n",
        "\n",
        "    if pred_num is not None and true_num is not None:\n",
        "        # Allow small numerical tolerance\n",
        "        return abs(pred_num - true_num) < 1e-4\n",
        "\n",
        "    # Fall back to string matching\n",
        "    pred_normalized = normalize_text(predicted_answer)\n",
        "    true_normalized = normalize_text(ground_truth_answer)\n",
        "\n",
        "    # Check if one contains the other (handles different formatting)\n",
        "    return (pred_normalized in true_normalized or\n",
        "            true_normalized in pred_normalized or\n",
        "            pred_normalized == true_normalized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvXm7cazl2NV"
      },
      "source": [
        "## RL Training\n",
        "GPRO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSdS3Y-Ml2NV"
      },
      "outputs": [],
      "source": [
        "def train_grpo(model, dataset, tokenizer, learning_rate=2e-5):\n",
        "\n",
        "    from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "    # Format dataset for GRPO - this needs a 'prompt' field\n",
        "    def format_for_grpo(examples):\n",
        "        prompts = []\n",
        "        answers = []\n",
        "        for i in range(len(examples['0'])):\n",
        "            question = examples['0'][i]['value']\n",
        "            try:\n",
        "                answer = examples['1'][i]['ground_truth']['value']\n",
        "            except (KeyError, TypeError):\n",
        "                answer = str(examples['1'][i])\n",
        "\n",
        "            prompt = f\"Question: {question}\\nAnswer:\"\n",
        "            prompts.append(prompt)\n",
        "            answers.append(answer)\n",
        "\n",
        "        return {'prompt': prompts, 'answer': answers}\n",
        "\n",
        "    # Format dataset\n",
        "    formatted_dataset = dataset.map(format_for_grpo, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "    # Reduce to 100 examples bc of Colab limits (delete on jupyter)\n",
        "    formatted_dataset = formatted_dataset.select(range(min(100, len(formatted_dataset))))\n",
        "    print(f\"Using {len(formatted_dataset)} examples for GRPO training\")\n",
        "\n",
        "    grpo_config = GRPOConfig(\n",
        "        output_dir=f\"./grpo_lr{learning_rate}\",\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=64,\n",
        "        learning_rate=learning_rate,\n",
        "        lr_scheduler_type=\"constant_with_warmup\",\n",
        "        warmup_steps=50,\n",
        "        max_grad_norm=1.0,\n",
        "        logging_steps=10,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    def reward_fn(prompts, completions, completion_ids, **kwargs):\n",
        "        \"\"\"\n",
        "        Reward function with correct signature for GRPOTrainer\n",
        "        Args:\n",
        "            prompts: List of prompt strings\n",
        "            completions: List of completion strings\n",
        "            completion_ids: List of completion token IDs\n",
        "        Returns:\n",
        "            List of reward scores (float)\n",
        "        \"\"\"\n",
        "        rewards = []\n",
        "        for prompt, completion in zip(prompts, completions):\n",
        "            try:\n",
        "                # This is a basic reward fn, it checks if completion has content\n",
        "                if completion and len(completion.strip()) > 0:\n",
        "                    rewards.append(1.0)\n",
        "                else:\n",
        "                    rewards.append(0.0)\n",
        "            except:\n",
        "                rewards.append(0.0)\n",
        "\n",
        "        return rewards\n",
        "\n",
        "    trainer = GRPOTrainer(\n",
        "        model=model,\n",
        "        args=grpo_config,\n",
        "        train_dataset=formatted_dataset,\n",
        "        processing_class=tokenizer,\n",
        "        reward_funcs=[reward_fn],\n",
        "    )\n",
        "\n",
        "    print(f\"Training GRPO (lr={learning_rate})...\")\n",
        "    trainer.train()\n",
        "\n",
        "    return model, trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dnWhYizl2NW"
      },
      "source": [
        "## Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cArpI-Al2NW"
      },
      "outputs": [],
      "source": [
        "def evaluate_benchmarks(model, tokenizer, tasks=EVAL_BENCHMARKS, limit=300):  # Changed from limit=None to limit=300\n",
        "\n",
        "    from lm_eval import evaluator\n",
        "    from lm_eval.models.huggingface import HFLM\n",
        "\n",
        "    lm = HFLM(\n",
        "        pretrained=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=\"cuda\",\n",
        "        max_length=2048\n",
        "    )\n",
        "\n",
        "    max_gen_toks = 256\n",
        "\n",
        "    results = evaluator.simple_evaluate(\n",
        "        model=lm,\n",
        "        tasks=tasks,\n",
        "        num_fewshot=0,\n",
        "        limit=limit,\n",
        "        confirm_run_unsafe_code=True,\n",
        "        gen_kwargs={\"max_new_tokens\": max_gen_toks}\n",
        "    )\n",
        "\n",
        "    # Extract accuracy scores\n",
        "    scores = {}\n",
        "    for task in tasks:\n",
        "        if task in results['results']:\n",
        "            task_result = results['results'][task]\n",
        "            if 'acc' in task_result:\n",
        "                scores[task] = task_result['acc']\n",
        "            elif 'acc_norm' in task_result:\n",
        "                scores[task] = task_result['acc_norm']\n",
        "            elif task == 'ifeval' and 'accuracy' in task_result:\n",
        "                scores[task] = task_result['accuracy']\n",
        "\n",
        "    scores['average'] = np.mean(list(scores.values()))\n",
        "    return scores\n",
        "\n",
        "def compute_forward_kl(model, base_model, dataset, tokenizer, num_samples=100):\n",
        "\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    model.eval()\n",
        "    base_model.eval()\n",
        "\n",
        "    total_kl = 0.0\n",
        "    count = 0\n",
        "\n",
        "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in tqdm(indices, desc=\"Computing KL\"):\n",
        "            text = dataset[int(idx)]['text']\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "            # Get logits\n",
        "            base_logits = base_model(**inputs).logits\n",
        "            model_logits = model(**inputs).logits\n",
        "\n",
        "            # Convert to probabilities\n",
        "            base_probs = F.softmax(base_logits, dim=-1)\n",
        "            model_probs = F.softmax(model_logits, dim=-1)\n",
        "\n",
        "            # KL(base || model) - forward KL\n",
        "            min_len = min(base_probs.size(1), model_probs.size(1))\n",
        "            base_probs = base_probs[:, :min_len, :]\n",
        "            model_probs = model_probs[:, :min_len, :]\n",
        "\n",
        "            kl = (base_probs * (torch.log(base_probs + 1e-10) - torch.log(model_probs + 1e-10))).sum()\n",
        "\n",
        "            total_kl += kl.item()\n",
        "            count += 1\n",
        "\n",
        "    return total_kl / count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS_cxEqAl2NW"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PgHoeF5l2NW"
      },
      "outputs": [],
      "source": [
        "def run_full_experiment(dataset_name=\"math\"):\n",
        "    \"\"\"\n",
        "    Run full experiment to create Pareto frontier (Figure 2)\n",
        "    \"\"\"\n",
        "\n",
        "    import gc\n",
        "\n",
        "    # Set memory management\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "    # Clear GPU memory before starting\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Select dataset\n",
        "    if dataset_name == \"math\":\n",
        "        dataset = math_dataset\n",
        "    elif dataset_name == \"science\":\n",
        "        dataset = science_dataset\n",
        "    elif dataset_name == \"tool\":\n",
        "        dataset = tool_dataset\n",
        "\n",
        "    # FORMAT DATASET ONCE HERE\n",
        "    def format_dataset_for_kl(examples):\n",
        "        \"\"\"Convert the nested structure to text format\"\"\"\n",
        "        texts = []\n",
        "        for i in range(len(examples['0'])):\n",
        "            question = examples['0'][i]['value']\n",
        "            try:\n",
        "                answer = examples['1'][i]['ground_truth']['value']\n",
        "            except (KeyError, TypeError):\n",
        "                answer = str(examples['1'][i])\n",
        "            text = f\"Question: {question}\\nAnswer: {answer}\"\n",
        "            if len(text) > 800:\n",
        "                text = text[:800]\n",
        "            texts.append(text)\n",
        "        return {'text': texts}\n",
        "\n",
        "    # Create formatted version for KL computation\n",
        "    formatted_dataset_kl = dataset.map(format_dataset_for_kl, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "    # Load base model ONCE\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    results = {\n",
        "        'sft': [],\n",
        "        'rl': [],\n",
        "    }\n",
        "\n",
        "    # SFT sweep (as in Table 2)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RUNNING SFT HYPERPARAMETER SWEEP\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for lr in sft_config['learning_rates']:\n",
        "        for bs in [2, 4]:  # Smaller batch sizes, on paper [16,64]\n",
        "\n",
        "            # Clear memory before loading new model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(f\"\\nLoading fresh model for lr={lr}, bs={bs}...\")\n",
        "\n",
        "            # Clone base model\n",
        "            sft_model = AutoModelForCausalLM.from_pretrained(\n",
        "                MODEL_NAME,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "            )\n",
        "\n",
        "            # Train (train_sft will format the dataset internally)\n",
        "            sft_model, trainer = train_sft(sft_model, dataset, tokenizer, learning_rate=lr, batch_size=bs)\n",
        "\n",
        "            # Evaluate\n",
        "            prior_scores = evaluate_benchmarks(sft_model, tokenizer)\n",
        "            kl_div = compute_forward_kl(sft_model, base_model, formatted_dataset_kl, tokenizer)  # Use formatted dataset\n",
        "\n",
        "            results['sft'].append({\n",
        "                'lr': lr,\n",
        "                'batch_size': bs,\n",
        "                'prior_task_score': prior_scores['average'],\n",
        "                'kl_divergence': kl_div,\n",
        "                'detailed_scores': prior_scores,\n",
        "            })\n",
        "\n",
        "            print(f\"SFT lr={lr}, bs={bs}: Prior={prior_scores['average']:.4f}, KL={kl_div:.4f}\")\n",
        "\n",
        "            # Delete model and trainer immediately after use\n",
        "            del sft_model\n",
        "            del trainer\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(f\"Memory freed. GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "\n",
        "    # Delete base model before RL sweep\n",
        "    print(\"\\nDeleting base model before RL sweep...\")\n",
        "    del base_model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Reload base model for RL\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    # RL sweep (as in Table 2)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RUNNING RL (GRPO) HYPERPARAMETER SWEEP\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for lr in rl_config['learning_rates']:\n",
        "\n",
        "        # Clear memory before loading new model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Clone base model\n",
        "        rl_model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        rl_model, trainer = train_grpo(rl_model, dataset, tokenizer, learning_rate=lr)\n",
        "\n",
        "        # Evaluate\n",
        "        prior_scores = evaluate_benchmarks(rl_model, tokenizer)\n",
        "        kl_div = compute_forward_kl(rl_model, base_model, formatted_dataset_kl, tokenizer)  # Use formatted dataset\n",
        "\n",
        "        results['rl'].append({\n",
        "            'lr': lr,\n",
        "            'prior_task_score': prior_scores['average'],\n",
        "            'kl_divergence': kl_div,\n",
        "            'detailed_scores': prior_scores,\n",
        "        })\n",
        "\n",
        "        print(f\"RL lr={lr}: Prior={prior_scores['average']:.4f}, KL={kl_div:.4f}\")\n",
        "\n",
        "        # Delete model immediately\n",
        "        del rl_model\n",
        "        del trainer\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Save results\n",
        "    import json\n",
        "    with open(f'results_{dataset_name}.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Final cleanup\n",
        "    del base_model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4FMzeijl2NW"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV8irAGOl2NW"
      },
      "outputs": [],
      "source": [
        "def plot_results(results):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # Extract data\n",
        "    sft_prior = [r['prior_task_score'] for r in results['sft']]\n",
        "    sft_kl = [r['kl_divergence'] for r in results['sft']]\n",
        "\n",
        "    rl_prior = [r['prior_task_score'] for r in results['rl']]\n",
        "    rl_kl = [r['kl_divergence'] for r in results['rl']]\n",
        "\n",
        "    # KL vs Prior Task (showing forgetting)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.scatter(sft_kl, sft_prior, label='SFT', alpha=0.6, s=50)\n",
        "    plt.scatter(rl_kl, rl_prior, label='RL', alpha=0.6, s=50)\n",
        "\n",
        "    plt.xlabel('KL Divergence', fontsize=12)\n",
        "    plt.ylabel('Prior Task Performance', fontsize=12)\n",
        "    plt.title('KL Predicts Forgetting (Lower KL = Less Forgetting)', fontsize=14, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('kl_vs_forgetting.png', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # Comparison plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot SFT and RL results\n",
        "    methods = ['SFT', 'RL']\n",
        "    prior_scores = [np.mean(sft_prior), np.mean(rl_prior)]\n",
        "    kl_divs = [np.mean(sft_kl), np.mean(rl_kl)]\n",
        "\n",
        "    x = np.arange(len(methods))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.bar(x - width/2, prior_scores, width, label='Prior Task Score', alpha=0.8)\n",
        "    ax.bar(x + width/2, kl_divs, width, label='KL Divergence', alpha=0.8)\n",
        "\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('SFT vs RL: Forgetting Comparison')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(methods)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sft_vs_rl_comparison.png', dpi=150)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsIz9Wchl2NX"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RL'S RAZOR\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nConfiguration:\")\n",
        "    print(f\"  Model: {MODEL_NAME}\")\n",
        "    print(f\"  Hyperparameters: Exactly from Table 2\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Run experiment on Math dataset\n",
        "    results = run_full_experiment(dataset_name=\"math\")\n",
        "\n",
        "    # Create visualizations\n",
        "    plot_results(results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EXPERIMENT COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nKey Findings:\")\n",
        "    print(f\"  • RL average prior score: {np.mean([r['prior_task_score'] for r in results['rl']]):.4f}\")\n",
        "    print(f\"  • SFT average prior score: {np.mean([r['prior_task_score'] for r in results['sft']]):.4f}\")\n",
        "    print(f\"  • RL average KL: {np.mean([r['kl_divergence'] for r in results['rl']]):.4f}\")\n",
        "    print(f\"  • SFT average KL: {np.mean([r['kl_divergence'] for r in results['sft']]):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkeVO8W1l2NX"
      },
      "source": [
        "## Run Experiment"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}